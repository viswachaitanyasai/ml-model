{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets pandas scikit-learn\n",
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready the Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('../review_data/dataset_7(senti).csv', on_bad_lines='skip', quoting=csv.QUOTE_ALL)\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "df = df.dropna(subset=['Rating', 'Review'])\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "def map_ratings_to_labels(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 0  # negative\n",
    "    elif rating == 3:\n",
    "        return 1  # neutral\n",
    "    elif rating in [4, 5]:\n",
    "        return 2  # positive\n",
    "\n",
    "df['labels'] = df['Rating'].apply(map_ratings_to_labels)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['labels'])\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Review'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize in batches\n",
    "batch_size = 1000\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, batch_size=batch_size)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True, batch_size=batch_size)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=20,\n",
    "#     per_device_train_batch_size=64,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     warmup_steps=2000,\n",
    "#     weight_decay=0.1,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=100,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=1e-4,\n",
    "#     fp16=True,  \n",
    "#     gradient_accumulation_steps=4,\n",
    "#     lr_scheduler_type=\"constant_with_warmup\",  \n",
    "#     save_total_limit=3, \n",
    "#     gradient_checkpointing=True,  \n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# Initialize the Trainer\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Test Accuracy: {eval_result['eval_accuracy']}\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trainded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../ml_trained_model/roberta_rating'\n",
    "file_path = os.path.join(directory)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created.\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(directory)\n",
    "tokenizer.save_pretrained(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e7eaffd4214865a93af99dd569f094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [16:43<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9760\n",
      "Testing F1 Score: 0.9729\n",
      "Testing Recall: 0.9760\n",
      "Testing Precision: 0.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('../review_data/dataset_7(senti).csv', on_bad_lines='skip', quoting=csv.QUOTE_ALL)\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Drop rows with missing values in 'Rating' and 'Review'\n",
    "df = df.dropna(subset=['Rating', 'Review'])\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "def map_ratings_to_labels(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 0  # negative\n",
    "    elif rating == 3:\n",
    "        return 1  # neutral\n",
    "    elif rating in [4, 5]:\n",
    "        return 2  # positive\n",
    "\n",
    "df['labels'] = df['Rating'].apply(map_ratings_to_labels)\n",
    "\n",
    "# Randomly select 500 reviews for testing\n",
    "test_df = df.sample(n=500, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../ml_trained_model/roberta_rating')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Review'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize in batches\n",
    "batch_size = 1000\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True, batch_size=batch_size)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Load the model\n",
    "model = RobertaForSequenceClassification.from_pretrained('../ml_trained_model/roberta_rating')\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=\"Evaluating\"):\n",
    "        inputs = {key: dataset[key][i].unsqueeze(0) for key in ['input_ids', 'attention_mask']}\n",
    "        labels = dataset['labels'][i].unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
    "        all_predictions.append(predictions[0])\n",
    "        all_labels.append(labels.numpy()[0])\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return accuracy, f1, recall, precision\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy, f1, recall, precision = evaluate(model, test_dataset)\n",
    "print(f\"Testing Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Testing F1 Score: {f1:.4f}\")\n",
    "print(f\"Testing Recall: {recall:.4f}\")\n",
    "print(f\"Testing Precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Accuracy: 0.9760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single line sentence Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../ml_trained_model/roberta_rating')\n",
    "model = RobertaForSequenceClassification.from_pretrained('../ml_trained_model/roberta_rating')\n",
    "\n",
    "sample_review = \"0% quality , bo touch work total pass waste phone\"\n",
    "\n",
    "# Tokenize the sample review\n",
    "inputs = tokenizer(sample_review, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Perform prediction\n",
    "outputs = model(**inputs)\n",
    "predictions = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
    "\n",
    "# Map numerical predictions back to sentiment labels\n",
    "label_map = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
    "predicted_sentiment = label_map[predictions[0]]\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array of sentence Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test strings\n",
    "test_strings = [\n",
    "    \"This product is amazing!\",\n",
    "    \"disappoint with this purchase\",\n",
    "    \"Value for money\",\n",
    "    \"bad\",\n",
    "    \"Great value for the price\",\n",
    "    \"Product worse\",\n",
    "    \"Sucks, I wanna die\",\n",
    "    \"I want to get another one its so good\",\n",
    "    \"Worse\",\n",
    "    \"sometim game answer question correctli alexa say got wrong answer like turn dont light away home\",\n",
    "    \"abl\",\n",
    "    \"Not bad\",\n",
    "    \"Good\",\n",
    "    \"Sure, the movie wasn't *awful*, but it was far from a masterpiece.\",\n",
    "    \"I can't believe they won the game! They totally choked in the last quarter, though.\",\n",
    "    \"Don't get me wrong, the food was good, but the service was painfully slow.\",\n",
    "    \"They say they improved the product, but I haven't noticed a difference yet.\",\n",
    "    \"Lucky me, I found a parking spot right in front of the store.\",\n",
    "    \"It's whatever. I guess the movie was okay.\",\n",
    "    \"That was a close one! Glad we pulled through in the end.\",\n",
    "    \"Eye roll. This new update is just a bunch of bugs.\",\n",
    "    \"Not bad for a first try! I can see potential here.\",\n",
    "    \"While the graphics were impressive, the story felt a bit lacking.\"\n",
    "]\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../ml_trained_model/roberta_rating')\n",
    "model = RobertaForSequenceClassification.from_pretrained('../ml_trained_model/roberta_rating')\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
    "    return predictions[0]\n",
    "\n",
    "# Map numerical predictions back to sentiment labels\n",
    "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Predict sentiment for each test string\n",
    "for text in test_strings:\n",
    "    sentiment_label = predict_sentiment(model, tokenizer, text)\n",
    "    predicted_sentiment = label_map[sentiment_label]\n",
    "    print(f\"Review: {text}\")\n",
    "    print(f\"Predicted sentiment: {predicted_sentiment}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
